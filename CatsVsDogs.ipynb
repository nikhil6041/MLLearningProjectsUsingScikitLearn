{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating different directories for training , validation and test data\n",
    "import os , shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "\n",
    "base_dir = 'D:\\Projects\\MLProjects\\Kaggle\\dogs-vs-cats-small'\n",
    "\n",
    "if os.path.isdir(base_dir) is not True:\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir , 'train')\n",
    "\n",
    "if os.path.isdir(train_dir) is not True:\n",
    "    os.mkdir(train_dir)\n",
    "\n",
    "validation_dir = os.path.join(base_dir , 'validation')\n",
    "\n",
    "if os.path.isdir(validation_dir) is not True:\n",
    "    os.mkdir(validation_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir , 'test')\n",
    "\n",
    "if os.path.isdir(test_dir) is not True:\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir , 'cats')\n",
    "\n",
    "if os.path.isdir(train_cats_dir) is not True:\n",
    "    os.mkdir(train_cats_dir)\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir , 'cats')\n",
    "\n",
    "if os.path.isdir(validation_cats_dir) is not True:\n",
    "    os.mkdir(validation_cats_dir)\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir , 'cats')\n",
    "\n",
    "if os.path.isdir(test_cats_dir) is not True:\n",
    "    os.mkdir(test_cats_dir)\n",
    "\n",
    "train_dogs_dir = os.path.join(train_dir , 'dogs')\n",
    "\n",
    "if os.path.isdir(train_dogs_dir) is not True:\n",
    "    os.mkdir(train_dogs_dir)\n",
    "\n",
    "validation_dogs_dir = os.path.join(validation_dir , 'dogs')\n",
    "\n",
    "if os.path.isdir(validation_dogs_dir) is not True:\n",
    "    os.mkdir(validation_dogs_dir)\n",
    "\n",
    "test_dogs_dir = os.path.join(test_dir , 'dogs')\n",
    "\n",
    "if os.path.isdir(test_dogs_dir) is not True:\n",
    "    os.mkdir(test_dogs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig_dir = os.path.join(original_dir,'train')\n",
    "\n",
    "## copying cat images\n",
    "\n",
    "filenames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in filenames:\n",
    "    source  = os.path.join(train_orig_dir,fname)\n",
    "    destination = os.path.join(train_cats_dir,fname)\n",
    "    shutil.copyfile(source,destination)\n",
    "\n",
    "    \n",
    "filenames = ['cat.{}.jpg'.format(i) for i in range(1000,1500)]\n",
    "for fname in filenames:\n",
    "    source  = os.path.join(train_orig_dir,fname)\n",
    "    destination = os.path.join(validation_cats_dir,fname)\n",
    "    shutil.copyfile(source,destination)\n",
    "    \n",
    "\n",
    "filenames = ['cat.{}.jpg'.format(i) for i in range(1500,2000)]\n",
    "for fname in filenames:\n",
    "    source  = os.path.join(train_orig_dir,fname)\n",
    "    destination = os.path.join(test_cats_dir,fname)\n",
    "    shutil.copyfile(source,destination)\n",
    "\n",
    "## copying dogs images    \n",
    "    \n",
    "filenames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in filenames:\n",
    "    source  = os.path.join(train_orig_dir,fname)\n",
    "    destination = os.path.join(train_dogs_dir,fname)\n",
    "    shutil.copyfile(source,destination)\n",
    "    \n",
    "filenames = ['dog.{}.jpg'.format(i) for i in range(1000,1500)]\n",
    "for fname in filenames:\n",
    "    source  = os.path.join(train_orig_dir,fname)\n",
    "    destination = os.path.join(validation_dogs_dir,fname)\n",
    "    shutil.copyfile(source,destination)\n",
    "\n",
    "filenames = ['dog.{}.jpg'.format(i) for i in range(1500,2000)]\n",
    "for fname in filenames:\n",
    "    source  = os.path.join(train_orig_dir,fname)\n",
    "    destination = os.path.join(test_dogs_dir,fname)\n",
    "    shutil.copyfile(source,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sample count for Cats\n",
      " Training Samples  1000\n",
      " Validation Samples  500\n",
      " Test Samples  500\n",
      " Sample count for Dogs\n",
      " Training Samples  1000\n",
      " Validation Samples  500\n",
      " Test Samples  500\n"
     ]
    }
   ],
   "source": [
    "### performing a cross check for the number of training , validation and test samples in our dataset\n",
    "\n",
    "def SamplesCount(dname): \n",
    "    return len(os.listdir(dname))\n",
    "\n",
    "\n",
    "print(\" Sample count for Cats\")\n",
    "\n",
    "print(\" Training Samples \",SamplesCount(train_cats_dir))\n",
    "\n",
    "print(\" Validation Samples \",SamplesCount(validation_cats_dir))\n",
    "\n",
    "print(\" Test Samples \",SamplesCount(test_cats_dir))\n",
    "\n",
    "print(\" Sample count for Dogs\")\n",
    "\n",
    "print(\" Training Samples \",SamplesCount(train_dogs_dir))\n",
    "\n",
    "print(\" Validation Samples \",SamplesCount(validation_dogs_dir))\n",
    "\n",
    "print(\" Test Samples \",SamplesCount(test_dogs_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constructing our ConvNet model\n",
    "\n",
    "from tensorflow.keras.layers import Dense , Flatten , Conv2D , MaxPooling2D , Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D( 32 , (3,3) , activation = 'relu' , input_shape = (150,150,3) ))\n",
    "\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D( 64 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D( 128 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D( 128 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Conv2D( 32 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512 , activation = 'relu' ))\n",
    "\n",
    "model.add(Dense(1 , activation = 'sigmoid' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile( loss = 'binary_crossentropy' ,optimizer = RMSprop(lr = 1e-4) , metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing our images before feeding them into the network\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        train_dir ,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                        validation_dir,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fitting our model on the generators \n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 25,\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps = 50\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs , acc , color = 'green' , label = 'Training accuracy')\n",
    "plt.plot(epochs , val_acc , color = 'orange' , label = 'Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs , loss , color = 'green' , label = 'Training loss')\n",
    "plt.plot(epochs , val_loss , color = 'orange' , label = 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### as can be seen from the graph plots our model is clearly overfitting as with each epoch our training loss \n",
    "### is reducing and training accuracy is increasing but on the other side our validation loss reaches its max of around 70%\n",
    "### and keeps oscillating there .Also the validation loss has a lot of noise in it along with being inconsistent in its nature\n",
    "### In order to mitigate this we can try data augmentation , dropout or other relevant techniques.\n",
    "### Since , data augmentation is a technique specific for a computer vision task we will try using it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            rotation_range = 40 ,\n",
    "            width_shift_range = 0.2,\n",
    "            height_shift_range = 0.2,\n",
    "            shear_range = 0.2,\n",
    "            zoom_range = 0.2,\n",
    "            horizontal_flip = True,\n",
    "            fill_mode = 'nearest'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualization of some augmented images\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "fnames = [ os.path.join(train_cats_dir , fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "img_path = fnames[143]\n",
    "\n",
    "img = image.load_img(img_path , target_size = (150,150))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for batch in datagen.flow(x,batch_size = 1):\n",
    "    plt.figure(cnt)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    cnt += 1\n",
    "    if cnt%4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## redifining our model\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D( 32 , (3,3) , activation = 'relu' , input_shape = (150,150,3) ))\n",
    "\n",
    "model2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model2.add(Conv2D( 64 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model2.add(Conv2D( 128 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model2.add(Conv2D( 128 , (3,3) , activation = 'relu'  ))\n",
    "\n",
    "model2.add(MaxPooling2D((2,2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(512 , activation = 'relu' ))\n",
    "\n",
    "model2.add(Dense(1 , activation = 'sigmoid' ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss = 'binary_crossentropy' , optimizer = RMSprop(lr = 1e-4 ) , metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen2 = ImageDataGenerator(\n",
    "                    rescale = 1./255 ,\n",
    "                    rotation_range = 40 ,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    horizontal_flip = True,\n",
    "                    fill_mode = 'nearest'\n",
    "                )\n",
    "\n",
    "test_datagen2 = ImageDataGenerator(\n",
    "                    rescale = 1./255\n",
    "                )\n",
    "\n",
    "train_generator2 = train_datagen2.flow_from_directory(\n",
    "                        train_dir ,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )\n",
    "\n",
    "validation_generator2 = test_datagen2.flow_from_directory(\n",
    "                        validation_dir,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fitting our model on the new generators \n",
    "history = model2.fit(\n",
    "            train_generator2,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 25,\n",
    "            validation_data = validation_generator2,\n",
    "            validation_steps = 50\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('cats_and_dogs_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs , acc , color = 'green' , label = 'Training accuracy')\n",
    "plt.plot(epochs , val_acc , color = 'orange' , label = 'Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs , loss , color = 'green' , label = 'Training loss')\n",
    "plt.plot(epochs , val_loss , color = 'orange' , label = 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the results obtained after applying data augmentation along with dropout are better but not at the mark . We would like \n",
    "### to do much better . So , now let's try using a pretrained model for our cats Vs dogs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using a pretrained VGG16 model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights = 'imagenet' , include_top = False , input_shape = (150 , 150 , 3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory , sample_count):\n",
    "    features = np.zeros(shape = (sample_count , 4 , 4 , 512))\n",
    "    labels = np.zeros(shape = (sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "                    directory,\n",
    "                    target_size = (150,150),\n",
    "                    batch_size = batch_size,\n",
    "                    class_mode = 'binary'\n",
    "                )\n",
    "    i = 0\n",
    "    for inputs_batch , labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i*batch_size : (i+1)*batch_size] = features_batch\n",
    "        labels[i*batch_size : (i+1)*batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i*batch_size >=  sample_count:\n",
    "            break\n",
    "            \n",
    "    return features , labels\n",
    "\n",
    "train_features , train_labels = extract_features(train_dir , 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features , validation_labels = extract_features(validation_dir , 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features , test_labels = extract_features(test_dir , 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features , (2000 , 4*4*512 ))\n",
    "validation_features = np.reshape(validation_features , (1000 , 4*4*512 ))\n",
    "test_features = np.reshape(test_features , (1000 , 4*4*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense( 256 , activation = 'relu' , input_dim = 4*4*512 ))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(1 , activation = 'sigmoid'))\n",
    "\n",
    "model3.compile(loss = 'binary_crossentropy' , optimizer = RMSprop(lr = 1e-4) , metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(\n",
    "                    train_features ,\n",
    "                    train_labels,\n",
    "                    epochs = 30,\n",
    "                    batch_size = 20,\n",
    "                    validation_data = (validation_features , validation_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history3.history['acc']\n",
    "val_acc = history3.history['val_acc']\n",
    "loss = history3.history['loss']\n",
    "val_loss = history3.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs , acc , color = 'green' , label = 'Training accuracy')\n",
    "plt.plot(epochs , val_acc , color = 'orange' , label = 'Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs , loss , color = 'green' , label = 'Training loss')\n",
    "plt.plot(epochs , val_loss , color = 'orange' , label = 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from the above method we can see that our accuracy has improved a lot both on training as well as validation set but while \n",
    "### looking at the losses we observe that we are still overfitting the given dataset so let's try to use some another technique\n",
    "### We will be doing feature extraction with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(conv_base)\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(256 , activation = 'relu'))\n",
    "model4.add(Dense(1 , activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "train_datagen3 = ImageDataGenerator(\n",
    "                    rescale = 1./255 ,\n",
    "                    rotation_range = 40 ,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    horizontal_flip = True,\n",
    "                    fill_mode = 'nearest'\n",
    "                )\n",
    "\n",
    "test_datagen3 = ImageDataGenerator(\n",
    "                    rescale = 1./255\n",
    "                )\n",
    "\n",
    "train_generator3 = train_datagen3.flow_from_directory(\n",
    "                        train_dir ,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )\n",
    "\n",
    "validation_generator3 = test_datagen3.flow_from_directory(\n",
    "                        validation_dir,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile( loss = 'binary_crossentropy' , optimizer = optimizers.RMSprop(lr = 2e-5) , metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fitting our model on the new generators \n",
    "history4 = model4.fit(\n",
    "            train_generator3,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 25,\n",
    "            validation_data = validation_generator3,\n",
    "            validation_steps = 50\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history4.history['acc']\n",
    "val_acc = history4.history['val_acc']\n",
    "loss = history4.history['loss']\n",
    "val_loss = history4.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs , acc , color = 'green' , label = 'Training accuracy')\n",
    "plt.plot(epochs , val_acc , color = 'orange' , label = 'Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs , loss , color = 'green' , label = 'Training loss')\n",
    "plt.plot(epochs , val_loss , color = 'orange' , label = 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile( loss = 'binary_crossentropy' , optimizer = optimizers.RMSprop(lr = 1e-5) , metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### fitting our model on the new generators \n",
    "history5 = model4.fit(\n",
    "            train_generator3,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 25,\n",
    "            validation_data = validation_generator3,\n",
    "            validation_steps = 50\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history5.history['acc']\n",
    "val_acc = history5.history['val_acc']\n",
    "loss = history5.history['loss']\n",
    "val_loss = history5.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs , acc , color = 'green' , label = 'Training accuracy')\n",
    "plt.plot(epochs , val_acc , color = 'orange' , label = 'Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs , loss , color = 'green' , label = 'Training loss')\n",
    "plt.plot(epochs , val_loss , color = 'orange' , label = 'Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('cats_and_dogs_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save('cats_and_dogs_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "                        test_dir ,\n",
    "                        target_size = (150,150),\n",
    "                        batch_size = 20,\n",
    "                        class_mode = 'binary'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss , test_acc = model.evaluate(test_generator , steps = 50)\n",
    "print(\"Test accuracy : ---> \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss , test_acc = model2.evaluate(test_generator , steps = 50)\n",
    "print(\"Test accuracy : ---> \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss , test_acc = model3.evaluate(test_features , test_labels , steps = 50)\n",
    "print(\"Test accuracy : ---> \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss , test_acc = model4.evaluate(test_generator , steps = 50)\n",
    "print(\"Test accuracy : ---> \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
